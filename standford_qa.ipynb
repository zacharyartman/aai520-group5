{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import json_normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>paragraphs.context</th>\n",
       "      <th>answer1</th>\n",
       "      <th>answer2</th>\n",
       "      <th>answer3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
       "      <td>56be4db0acb8001400a502ec</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Denver Broncos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
       "      <td>56be4db0acb8001400a502ed</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where did Super Bowl 50 take place?</td>\n",
       "      <td>56be4db0acb8001400a502ee</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>Levi's Stadium</td>\n",
       "      <td>Levi's Stadium in the San Francisco Bay Area a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which NFL team won Super Bowl 50?</td>\n",
       "      <td>56be4db0acb8001400a502ef</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Denver Broncos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What color was used to emphasize the 50th anni...</td>\n",
       "      <td>56be4db0acb8001400a502f0</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>gold</td>\n",
       "      <td>gold</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Which NFL team represented the AFC at Super Bo...   \n",
       "1  Which NFL team represented the NFC at Super Bo...   \n",
       "2                Where did Super Bowl 50 take place?   \n",
       "3                  Which NFL team won Super Bowl 50?   \n",
       "4  What color was used to emphasize the 50th anni...   \n",
       "\n",
       "                         id          title  \\\n",
       "0  56be4db0acb8001400a502ec  Super_Bowl_50   \n",
       "1  56be4db0acb8001400a502ed  Super_Bowl_50   \n",
       "2  56be4db0acb8001400a502ee  Super_Bowl_50   \n",
       "3  56be4db0acb8001400a502ef  Super_Bowl_50   \n",
       "4  56be4db0acb8001400a502f0  Super_Bowl_50   \n",
       "\n",
       "                                  paragraphs.context                  answer1  \\\n",
       "0  Super Bowl 50 was an American football game to...           Denver Broncos   \n",
       "1  Super Bowl 50 was an American football game to...        Carolina Panthers   \n",
       "2  Super Bowl 50 was an American football game to...  Santa Clara, California   \n",
       "3  Super Bowl 50 was an American football game to...           Denver Broncos   \n",
       "4  Super Bowl 50 was an American football game to...                     gold   \n",
       "\n",
       "             answer2                                            answer3  \n",
       "0     Denver Broncos                                     Denver Broncos  \n",
       "1  Carolina Panthers                                  Carolina Panthers  \n",
       "2     Levi's Stadium  Levi's Stadium in the San Francisco Bay Area a...  \n",
       "3     Denver Broncos                                     Denver Broncos  \n",
       "4               gold                                               gold  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read json data tp json_data\n",
    "json_data = pd.read_json('data/dev-v1.1.json')\n",
    "\n",
    "# Use json_normalize to flatten question and id, while keeping answers\n",
    "df = json_normalize(\n",
    "    json_data['data'], \n",
    "    record_path=['paragraphs', 'qas'], \n",
    "    meta=['title', ['paragraphs', 'context']], \n",
    "    errors='ignore'\n",
    ")\n",
    "\n",
    "# Extract answers and create separate columns for answer1, answer2, answer3\n",
    "df[['answer1', 'answer2', 'answer3']] = pd.DataFrame(\n",
    "    df['answers'].apply(lambda ans: [answer['text'] for answer in ans[:3]]).to_list(), index=df.index\n",
    ")\n",
    "\n",
    "# Drop the original 'answers' column\n",
    "df = df.drop(columns=['answers'])\n",
    "\n",
    "# Display the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Column that combines questions and context\n",
    "df['input_text'] = df.apply(lambda row: f\"Context: {row['paragraphs.context']} Question: {row['question']} Answer:\", axis=1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(df[['input_text', 'answer1']], test_size=0.2)\n",
    "\n",
    "# Convert to list of dictionaries for training\n",
    "train_data = [{'input_text': row['input_text'], 'target_text': row['answer1']} for idx, row in train_data.iterrows()]\n",
    "val_data = [{'input_text': row['input_text'], 'target_text': row['answer1']} for idx, row in val_data.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "model_name = 't5-small' \n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Using a dataset object\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "# Tokenizing the input and target\n",
    "def preprocess_data(examples):\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\").input_ids\n",
    "    \n",
    "    # Replacing padding token ids in labels with -100 to ignore them\n",
    "    labels_with_padding = [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "    model_inputs['labels'] = labels_with_padding\n",
    "    return model_inputs\n",
    "\n",
    "# Applying preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./trained_model')\n",
    "tokenizer.save_pretrained('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae7dfbf04764b67829fa413749aeefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bb336e2daa436ab88ba8e8b9cc0eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/529 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.029073098674416542, 'eval_model_preparation_time': 0.0016, 'eval_runtime': 70.6523, 'eval_samples_per_second': 29.921, 'eval_steps_per_second': 7.487}\n",
      "Input: Context: The Rankine cycle is the fundamental thermodynamic underpinning of the steam engine. The cycle is an arrangement of components as is typically used for simple power production, and utilizes the phase change of water (boiling water producing steam, condensing exhaust steam, producing liquid water)) to provide a practical heat/power conversion system. The heat is supplied externally to a closed loop with some of the heat added being converted to work and the waste heat being removed in a condenser. The Rankine cycle is used in virtually all steam power production applications. In the 1990s, Rankine steam cycles generated about 90% of all electric power used throughout the world, including virtually all solar, biomass, coal and nuclear power plants. It is named after William John Macquorn Rankine, a Scottish polymath. Question: What happens to waste heat in the Rankine cycle? Answer:\n",
      "Predicted Answer: removed in a condenser\n",
      "Actual Answer: removed in a condenser\n",
      "--------------------------------------------------\n",
      "Input: Context: In some rural areas in the United Kingdom, there are dispensing physicians who are allowed to both prescribe and dispense prescription-only medicines to their patients from within their practices. The law requires that the GP practice be located in a designated rural area and that there is also a specified, minimum distance (currently 1.6 kilometres) between a patient's home and the nearest retail pharmacy. This law also exists in Austria for general physicians if the nearest pharmacy is more than 4 kilometers away, or where none is registered in the city. Question: What is the minimum distance between a patient's home and the nearest pharmacy that allows a physician in Austria to give out medicine? Answer:\n",
      "Predicted Answer: 1.6 kms\n",
      "Actual Answer: more than 4 kilometers\n",
      "--------------------------------------------------\n",
      "Input: Context: The V&A holds over 19,000 items from the Islamic world, ranging from the early Islamic period (the 7th century) to the early 20th century. The Jameel Gallery of Islamic Art, opened in 2006, houses a representative display of 400 objects with the highlight being the Ardabil Carpet, the centrepiece of the gallery. The displays in this gallery cover objects from Spain, North Africa, the Middle East, Central Asia and Afghanistan. A masterpiece of Islamic art is a 10th-century Rock crystal ewer. Many examples of Qur'Äns with exquisite calligraphy dating from various periods are on display. A 15th-century minbar from a Cairo mosque with ivory forming complex geometrical patterns inlaid in wood is one of the larger objects on display. Extensive examples of ceramics especially Iznik pottery, glasswork including 14th-century lamps from mosques and metalwork are on display. The collection of Middle Eastern and Persian rugs and carpets is amongst the finest in the world, many were part of the Salting Bequest of 1909. Examples of tile work from various buildings including a fireplace dated 1731 from Istanbul made of intricately decorated blue and white tiles and turquoise tiles from the exterior of buildings from Samarkand are also displayed. Question: What is considered the centerpiece of the Jameel Gallery of Islamic Art? Answer:\n",
      "Predicted Answer: Ardabil Carpet\n",
      "Actual Answer: Ardabil Carpet\n",
      "--------------------------------------------------\n",
      "Input: Context: Various gold-themed promotions and initiatives were held throughout the 2015 NFL season to tie into the \"Golden Super Bowl\"; gold-tinted logos were implemented across the NFL's properties and painted on fields, the numbering of the 50-yard line on fields was colored gold, and beginning on week 7, all sideline jackets and hats featured gold-trimmed logos. Gold footballs were given to each high school that has had a player or coach appear in the Super Bowl, and \"homecoming\" events were also held by Super Bowl-winning teams at games. Question: What was given to high schools where former students went on to play or coach in a Super Bowl? Answer:\n",
      "Predicted Answer: Gold footballs\n",
      "Actual Answer: Gold footballs\n",
      "--------------------------------------------------\n",
      "Input: Context: What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as  and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of  will experience a force: Question: Where was the measurment for the standard gravity on Earth taken? Answer:\n",
      "Predicted Answer: sea level\n",
      "Actual Answer: sea level\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model and tokenizer\n",
    "model_path = './app/trained_model'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Prepare the validation dataset for evaluation (assuming `val_data` is already prepared)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "# Tokenizing the input and target\n",
    "def preprocess_data(examples):\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\").input_ids\n",
    "    \n",
    "    # Replacing padding token ids in labels with -100 to ignore them during loss computation\n",
    "    labels_with_padding = [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "    model_inputs['labels'] = labels_with_padding\n",
    "    return model_inputs\n",
    "\n",
    "# Applying preprocessing on validation dataset\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Define evaluation arguments\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Create a trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "# Generate predictions for the validation set\n",
    "def generate_answer(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated output\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Test the model on some validation samples\n",
    "for i in range(5):  # Adjust to test more examples\n",
    "    sample = val_data[i]\n",
    "    input_text = sample['input_text']\n",
    "    target_text = sample['target_text']\n",
    "    \n",
    "    predicted_answer = generate_answer(input_text)\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    print(f\"Actual Answer: {target_text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "The task involved fine-tuning a T5-based model for question answering using a dataset containing contexts, questions, and answers. The steps for training and evaluation were as follows:\n",
    "\n",
    "#### Data Preparation:\n",
    "\n",
    "- A JSON dataset was processed and flattened to extract contexts, questions, and answers.\n",
    "- The data was further split into training and validation sets, where each row contained an input_text (combining context and question) and a target (answer1).\n",
    "\n",
    "\n",
    "#### Model and Tokenization:\n",
    "\n",
    "- The T5Tokenizer and T5ForConditionalGeneration model were used from the Hugging Face transformers library.\n",
    "Both the training and validation datasets were tokenized, with inputs truncated to a maximum length of 512 tokens and target sequences to 64 tokens.\n",
    "\n",
    "#### Model Setup and Training: \n",
    "\n",
    "- The Trainer class was employed for training, using a batch size of 4, with evaluation strategy set to run every 10 steps. Training was resumed from a checkpoint.\n",
    "The model was trained for 1 epoch with num_train_epochs=1.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "- We evaluated the model was evaluated using the validation dataset after training, calculating the loss and checking predictions against actual answers.\n",
    "\n",
    "### Results\n",
    "- Evaluation Loss: The model achieved a very low evaluation loss of 0.029.\n",
    "- Efficiency: The model processed 29.92 samples per second and 7.49 steps per second during evaluation.\n",
    "- Example Prediction: On one sample from the validation set, the model was given the context of the Rankine cycle and asked, \"What happens to waste heat in the Rankine cycle?\"\n",
    "  - Predicted Answer: \"removed in a condenser\"\n",
    "  - Actual Answer: \"removed in a condenser\"\n",
    "\n",
    "The model's performance on this example was highly accurate, demonstrating effective learning and prediction capabilities for T5 model for the question-answering task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
