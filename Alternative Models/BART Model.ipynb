{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8f0739-4330-4129-b04c-b62877ecfb52",
   "metadata": {},
   "source": [
    "# BART Model\n",
    "\n",
    "A transformer model combining BERT's text comprehension and GPT's text generation, ideal for summarization, translation, and question-answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44198128-9604-43c6-a38e-6b6412f4e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Initialize the BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# Preprocessing function for tokenization\n",
    "def preprocess_function(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    # Iterate over the batched data\n",
    "    for i in range(len(examples['context'])):\n",
    "        context = examples['context'][i]  # Access each context in the batch\n",
    "        question = examples['question'][i]  # Access each question in the batch\n",
    "        answer = examples['answer'][i]  # Access each answer in the batch\n",
    "\n",
    "        # Combine the context and question as input\n",
    "        inputs.append(f\"Context: {context} Question: {question}\")\n",
    "        targets.append(answer)  # Use the answer as the target\n",
    "\n",
    "    # Tokenize the inputs (contexts + questions) with padding and truncation\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=1024, truncation=True, padding='max_length'  # Ensure uniform length\n",
    "    )\n",
    "\n",
    "    # Tokenize the targets (answers) with padding and truncation\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, max_length=128, truncation=True, padding='max_length'  # Ensure uniform length\n",
    "        )\n",
    "\n",
    "    # Fix the nested list issue by using 'input_ids' directly\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "875a80e7-ef01-4143-a448-b8b144b41bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6addd2b4f64b9aa406b50238c6a4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba55a9cea0934d8ba3116da7013d156c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SColbe01\\AppData\\Local\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset and ensure caching is disabled\n",
    "dataset = load_dataset('json', data_files={'train': 'sb1.json'})\n",
    "# Apply the preprocessing function with caching completely disabled\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    load_from_cache_file=False, \n",
    "    keep_in_memory=True  # Ensure the dataset is processed in memory without cache files\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf65ab3a-d8d1-4666-bf0a-7eb392662c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SColbe01\\AppData\\Local\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the model and tokenizer from local paths\n",
    "tokenizer = BartTokenizer.from_pretrained('bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained('bart-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9007fffe-488b-4bf0-b7b5-f5d59dfe2851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 33:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.661262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.458136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.066216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed, and the fine-tuned BART model has been saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',              # Output directory\n",
    "    eval_strategy='epoch',         # Evaluate once per epoch\n",
    "    learning_rate=5e-5,                  # Learning rate\n",
    "    per_device_train_batch_size=4,       # Training batch size\n",
    "    per_device_eval_batch_size=4,        # Evaluation batch size\n",
    "    num_train_epochs=3,                  # Number of epochs\n",
    "    weight_decay=0.01,                   # Weight decay\n",
    "    save_steps=500,                      # Save every 500 steps\n",
    "    logging_dir='./logs',                # Logging directory\n",
    "    logging_steps=100                    # Log every 100 steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the model, tokenizer, and the training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,                         # Use the model defined in your code\n",
    "    args=training_args,                  # Use the training arguments defined above\n",
    "    train_dataset=tokenized_datasets['train'],  # Use your tokenized training dataset\n",
    "    eval_dataset=tokenized_datasets['train'],   # Evaluation dataset (could replace with a separate dataset)\n",
    "    tokenizer=tokenizer                  # Use your tokenizer\n",
    ")\n",
    "\n",
    "# Start training the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer for future use\n",
    "model.save_pretrained('./fine_tuned_bart_model')  # Save the model to the specified directory\n",
    "tokenizer.save_pretrained('./fine_tuned_bart_model')  # Save the tokenizer\n",
    "\n",
    "print(\"Training completed, and the fine-tuned BART model has been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11ae6b2d-3cae-493d-9fda-8905cd0ef7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with your BART model! (type 'exit' to stop)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  Which NFL team represented the AFC at Super Bowl 50?\n",
      "Question:  Which NFL team represented the AFC at Super Bowl 50?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  Superbowl\n",
      "Question:  when is the superbowl?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  exit\n",
      "Question:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting chat.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb88eaf1-4232-4d65-874e-684fca9b44d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with your BART model! (type 'exit' to stop)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50\n",
      "Question:  Which NFL team represented the AFC at Super Bowl 50?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Denver Broncos\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question:  which team represented AFC \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Denver Broncos\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\"\n",
      "Question:  What venue did Super Bowl 50 take place in?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Denver Broncos\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\"\n",
      "Question:  What venue did Super Bowl 50 take place in?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\"\n",
      "Question:  What year did the Denver Broncos secure a Super Bowl title for the third time?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Denver Broncos\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  \"On May 21, 2013, NFL owners at their spring meetings in Boston voted and awarded the game to Levi's Stadium. The $1.2 billion stadium opened in 2014. It is the first Super Bowl held in the San Francisco Bay Area since Super Bowl XIX in 1985, and the first in California since Super Bowl XXXVII took place in San Diego in 2003.\"\n",
      "Question:  Where did the spring meetings of the NFL owners take place?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2014\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Context:  exit\n",
      "Question:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting chat.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Step 1: Load the fine-tuned BART model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained('./fine_tuned_bart_model')\n",
    "tokenizer = BartTokenizer.from_pretrained('./fine_tuned_bart_model')\n",
    "\n",
    "# Step 2: Define a chat function with debugging\n",
    "def chat_with_model():\n",
    "    print(\"Chat with your BART model! (type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        context = input(\"Context: \")\n",
    "        question = input(\"Question: \")\n",
    "        \n",
    "        if context.lower() == 'exit' or question.lower() == 'exit':\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        \n",
    "        # Prepare the input for the model\n",
    "        input_text = f\"Context: {context} Question: {question}\"\n",
    "        # print(f\"Input Text: {input_text}\")  # Debugging to check input format\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
    "        \n",
    "        # Check if the inputs are tokenized correctly\n",
    "        # print(f\"Tokenized Input: {inputs}\")  # Debugging to check tokenization\n",
    "\n",
    "        # Generate the model's response\n",
    "        outputs = model.generate(inputs['input_ids'], max_length=50, num_beams=5, early_stopping=True)\n",
    "        \n",
    "        # Decode and print the model's response\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "# Step 3: Start the chat\n",
    "chat_with_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b48aa9-40fe-4e6d-837d-1a18c6d707df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
